{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHTeamAzhGNBPhOcjRlj1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a6eaaa6bb3f94d0087d602556493cd83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f50404ab9af4108b72efa03dc317ebe",
              "IPY_MODEL_46a9c3d57594486dbd4345a2d47706b8",
              "IPY_MODEL_5057fda74979449cab246a26c79d85e7"
            ],
            "layout": "IPY_MODEL_a66ccba881fd476b9cb38f84ab38ecb3"
          }
        },
        "6f50404ab9af4108b72efa03dc317ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f85259255a744adc8736cc2d690cad1d",
            "placeholder": "​",
            "style": "IPY_MODEL_82fc1e207944414ca5e518a2940d28e2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "46a9c3d57594486dbd4345a2d47706b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_037d96ce44c84dd78a2f835d1357568c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0ef0fb6dfbf49c7b27f59a163555ebc",
            "value": 2
          }
        },
        "5057fda74979449cab246a26c79d85e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b51ee041304e77a4583247beb00a97",
            "placeholder": "​",
            "style": "IPY_MODEL_03835a9bb3cc46519dd6b312fdb66b76",
            "value": " 2/2 [00:39&lt;00:00, 18.99s/it]"
          }
        },
        "a66ccba881fd476b9cb38f84ab38ecb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f85259255a744adc8736cc2d690cad1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82fc1e207944414ca5e518a2940d28e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "037d96ce44c84dd78a2f835d1357568c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0ef0fb6dfbf49c7b27f59a163555ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4b51ee041304e77a4583247beb00a97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03835a9bb3cc46519dd6b312fdb66b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethvedbitdesjan/AI-ML_Training/blob/main/LLMIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npvFWV7JZ2Sw",
        "outputId": "c9eab1d2-1609-4fd3-c057-e116b642d08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKcKMuIew29f",
        "outputId": "d5e8c58f-d00a-4b5f-aef6-7a12a7a26cce"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m204.8/227.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "from google.colab import userdata\n",
        "huggingface_hub.login(userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbDMR1Ytasz_",
        "outputId": "a71edcb8-d86e-4004-aace-457cd269c584"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "a6eaaa6bb3f94d0087d602556493cd83",
            "6f50404ab9af4108b72efa03dc317ebe",
            "46a9c3d57594486dbd4345a2d47706b8",
            "5057fda74979449cab246a26c79d85e7",
            "a66ccba881fd476b9cb38f84ab38ecb3",
            "f85259255a744adc8736cc2d690cad1d",
            "82fc1e207944414ca5e518a2940d28e2",
            "037d96ce44c84dd78a2f835d1357568c",
            "b0ef0fb6dfbf49c7b27f59a163555ebc",
            "b4b51ee041304e77a4583247beb00a97",
            "03835a9bb3cc46519dd6b312fdb66b76"
          ]
        },
        "id": "vgVNgOjvaIFl",
        "outputId": "0aa63a1b-29ef-482d-e272-98f1e1a7b5aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.c1358f8a35e6d2af81890deffbbfa575b978c62f.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.c1358f8a35e6d2af81890deffbbfa575b978c62f.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6eaaa6bb3f94d0087d602556493cd83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_response(prompt, max_new_tokens=100, temperature=1.0):\n",
        "    assert temperature > 0.0, \"Temperature must be greater than 0.0\"\n",
        "    assert max_new_tokens > 0, \"Max new tokens must be greater than 0\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
        "    ]\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"return_full_text\": False,\n",
        "        \"temperature\": temperature,\n",
        "        \"do_sample\": True,\n",
        "    }\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    return output[0]['generated_text']"
      ],
      "metadata": {
        "id": "ateymyHXmk6F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(get_model_response(\"Write poem on machine learning\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "htAfkWWMkc1Y",
        "outputId": "91c6eeee-49a6-415c-eb70-a488f4bd1959"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " **The Dance of Neurons**\n\n\nIn silicon groves where data doth wind,\n\nAlgorithms in whispers, to insights they inclined.\n\nA field of stars – a neural net – does sprawl,\n\nEach node a neuron, in memory they recall.\n\n\nBackpropagation, a path it seeks,\n\nTo tune the weights where knowledge peaks.\n\nThrough layers deep where hidden"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs and LLM Modulo Frameworks: A Comprehensive Introduction"
      ],
      "metadata": {
        "id": "GVpz_YpOnlpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## I. Introduction to Large Language Models (LLMs)"
      ],
      "metadata": {
        "id": "aspLKDjyn4J0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7YXi_ZlNn-9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### A. Definition of LLMs\n",
        "llm_definition_prompt = \"Provide a concise definition of Large Language Models (LLMs).\"\n",
        "llm_definition = get_model_response(llm_definition_prompt, max_new_tokens=50)\n",
        "display(Markdown(f\"**LLM Definition:** {llm_definition}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "ED1x5PgHnsPJ",
        "outputId": "a0c1bd50-0d99-4f78-9892-ed2da2706bbb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**LLM Definition:**  Large Language Models (LLMs) are sophisticated artificial intelligence tools that can understand and generate human language from a vast dataset. They are capable of processing, contextualizing, and predicting text based on patterns they've"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### B. Brief overview of how LLMs work\n",
        "\n",
        "# %%\n",
        "llm_overview_prompt = \"Explain in simple terms how Large Language Models work.\"\n",
        "llm_overview = get_model_response(llm_overview_prompt, max_new_tokens=100)\n",
        "display(Markdown(f\"**How LLMs Work:** {llm_overview}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "tj-Kdxx4oFda",
        "outputId": "55495d2d-fcbe-418d-a843-dc11aaa56ecd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**How LLMs Work:**  Large Language Models, or LLMs, are sophisticated computer programs that understand and generate human-like text. Think of them like advanced spellers and writers combined. They digest huge amounts of written words and texts to learn how language works — from basic grammar to more complex conversational cues. Once trained, these models can mimic text so well that they can compose essays, write stories, converse with users, and even translate languages. They're"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### C. Real-world applications and examples\n",
        "\n",
        "# %%\n",
        "applications_prompt = \"List 5 real-world applications of Large Language Models.\"\n",
        "applications = get_model_response(applications_prompt, max_new_tokens=150)\n",
        "display(Markdown(f\"**Real-world Applications of LLMs:**\\n{applications}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Gv6_pvNJoQpE",
        "outputId": "1b484bde-95da-46ef-b181-7df8a464d04f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Real-world Applications of LLMs:**\n Large language models (LLMs) have become increasingly prevalent in a variety of real-world applications. Here are five notable ones:\n\n1. **Content Generation:** LLMs can generate a wide variety of content, including written pieces, such as articles, blog posts, social media content, reports, and scripts. For example, Microsoft's GPT series of models can generate human-like articles on a wide array of topics or assist in marketing content creation.\n\n2. **Natural Language Understanding (NLU):** LLMs play a significant role in understanding human language and are used for various tasks such as sentiment analysis, machine translation, chatbots, and voice assistants"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### A. Natural language processing tasks\n",
        "\n",
        "# %%\n",
        "nlp_tasks = [\n",
        "    \"Writing and composition\",\n",
        "    \"Paraphrasing and summarization\",\n",
        "    \"Translation between languages\"\n",
        "]\n",
        "\n",
        "for task in nlp_tasks:\n",
        "    prompt = f\"Demonstrate the capability of an LLM in {task}. Provide a short example.\"\n",
        "    response = get_model_response(prompt, max_new_tokens=100)\n",
        "    display(Markdown(f\"**{task}:**\\n{response}\\n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "yJyUk0wyoTO6",
        "outputId": "0e7e0e45-418b-4819-e975-55f6a7d1e9da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Writing and composition:**\n One of the most powerful aspects of large language models (LLMs) such as me is their ability to write and compose text. Here's a short example of an LLM's writing and composition skills, focusing on creating an engaging story prompt:\n\n---\nIn a small, humble village nestled at the foot of a snow-capped mountain range, there lived an old couple named Tom and Anna. Despite their age, they carried with them a twink\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Paraphrasing and summarization:**\n Here's an example of paraphrasing and summarization using an LLM, given the input text below:\n\n```\nOriginally, in 1972, scientists discovered the HIV virus that leads to AIDS. Despite their numerous efforts, there's no cure yet. While antiretroviral treatment has significantly extended the lives of HIV patients, the long-term battle against this disease continues as we strive to achieve a c\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Translation between languages:**\n Certainly! For demonstration, let's translate a short sentence from English to French using Microsoft's Translator. Please note that while this example will not produce results from a live Large Language Model (LLM), the method depicted simulates the process on Microsoft's platform at the time of writing.\n\n**Original English Sentence:**\n\"The quick brown fox jumps over the lazy dog.\"\n\nBased on Microsoft's capabilities\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### B. Information retrieval and synthesis\n",
        "\n",
        "# %%\n",
        "info_synthesis_prompt = \"Explain how LLMs excel at information retrieval and synthesis. Give an example.\"\n",
        "info_synthesis = get_model_response(info_synthesis_prompt, max_new_tokens=150)\n",
        "display(Markdown(f\"**Information Retrieval and Synthesis:**\\n{info_synthesis}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "jdu1SaS4oZEY",
        "outputId": "af24f208-e452-4754-81d9-32e5ec906e2a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Information Retrieval and Synthesis:**\n Large language models (LLMs) are highly efficient at information retrieval and synthesis due to their deep understanding of natural language and sophisticated algorithms that enable them to process and analyze vast amounts of textual data. These models have the ability to comprehend the context, nuances, and underlying connections between various pieces of information, which allows them to retrieve highly relevant and accurate information based on users' queries. Moreover, they can assimilate and integrate multiple sources of information, producing coherent, synthesized outputs that are valuable for decision-making, learning, and problem-solving.\n\nFor instance, suppose a researcher wants to investigate the potential health impacts of vaping. The researcher could"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### C. Pattern recognition in text\n",
        "\n",
        "# %%\n",
        "pattern_recognition_prompt = \"Describe how LLMs perform pattern recognition in text. Provide an example.\"\n",
        "pattern_recognition = get_model_response(pattern_recognition_prompt, max_new_tokens=150)\n",
        "display(Markdown(f\"**Pattern Recognition in Text:**\\n{pattern_recognition}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "lKw4FapBoa3U",
        "outputId": "093da4d6-dac9-4147-e3e7-259d000259e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Pattern Recognition in Text:**\n Large Language Models (LLMs) like GPT (Generative Pre-trained Transformer) perform pattern recognition in text through various processes inherent in their architecture and training. At their core, LLMs leverage complex neural networks that can process sequential data (text) to recognize patterns over different degrees of granularity, from individual words (token recognition) to entire sentence constructions (contextual understanding).\n\n\nA crucial component of this process is the use of transformers, which replace earlier sequential methods like RNNs and LSTMs. Transformers allow for parallel processing across the entire text and significantly reduce the computational time required for training these models.\n\n\nOne of the"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### D. In-context learning and task adaptation\n",
        "\n",
        "# %%\n",
        "adaptation_prompt = \"Explain in-context learning and task adaptation in LLMs. Give an example.\"\n",
        "adaptation = get_model_response(adaptation_prompt, max_new_tokens=150)\n",
        "display(Markdown(f\"**In-context Learning and Task Adaptation:**\\n{adaptation}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "iRnu4Iczoczd",
        "outputId": "eeaa2cdd-696b-4c0b-bf8a-843d308cc15b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**In-context Learning and Task Adaptation:**\n In-context learning and task adaptation in LLMs (Large Language Models) refer to the ability of the model to understand and perform specific tasks based on the context provided within an input. This means that the LLM can learn a new task without being explicitly programmed for that specific task, but rather by understanding the context provided in the input.\n\nFor example, imagine you are providing an LLM with a dataset of customer service emails. The model will examine the input, analyze the keywords and patterns used, and understand that it needs to process or respond to customer queries. You might then input a particular example email, and the model will identify itself as a customer service representative, and use the knowledge it gained from the dataset to"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### E. Demonstration: Show a live example of an LLM performing a writing or summarization task\n",
        "\n",
        "# %%\n",
        "demo_text = \"\"\"\n",
        "The Internet of Things (IoT) refers to the interconnected network of physical devices, vehicles, home appliances, and other items embedded with electronics, software, sensors, and network connectivity, which enables these objects to collect and exchange data. IoT has applications in various fields including smart homes, healthcare, agriculture, and industrial automation. It promises to make our lives more efficient and convenient, but also raises concerns about privacy and security.\n",
        "\"\"\"\n",
        "\n",
        "demo_prompt = f\"Summarize the following text in 3 bullet points:\\n\\n{demo_text}\"\n",
        "summary = get_model_response(demo_prompt, max_new_tokens=200)\n",
        "display(Markdown(f\"**Summary of IoT Text:**\\n{summary}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "qgWCvAjCoejZ",
        "outputId": "b4aead7f-ebf7-4bb8-a9cb-3c343e986a59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Summary of IoT Text:**\n - The Internet of Things (IoT) is a network of interconnected devices, such as those found in homes, vehicles, appliances, and other items, equipped with advanced capabilities to collect, exchange, and analyze data.\n\n- IoT technology spans numerous sectors like smart home management, healthcare solutions, agricultural advancements, and industrial process automation, offering the potential to enhance personal comfort, health monitoring, crop management, and production efficiency.\n\n- While IoT has the potential to revolutionize convenience and efficiency, it also brings challenges in terms of protecting user privacy and safeguarding against cyber threats, necessitating robust security measures."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## II. Tokenization"
      ],
      "metadata": {
        "id": "NFjWN0x7o8hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are Tokens?**\n",
        "\n",
        "In the realm of Natural Language Processing (NLP), tokens are the fundamental building blocks of text analysis.\n",
        "Think of them as the individual units of meaning that make up a sentence.\n",
        "These units can be as simple as individual words like \"cat,\" \"dog,\" or \"run,\"\n",
        "or they can be more complex elements like punctuation marks (\".\" \",\" \"!\") or even subword units.\n",
        "\n",
        "\n",
        "**Why Do We Need Tokenizers?**\n",
        "\n",
        "Computers excel at processing numbers, not raw text.\n",
        "Tokenizers bridge this gap by converting human-readable text into a numerical representation that machine learning models can understand.\n",
        "This process is crucial for various NLP tasks, including:\n",
        "   - **Text Classification:** Categorizing text into predefined categories (e.g., spam detection).\n",
        "   - **Machine Translation:** Converting text from one language to another.\n",
        "   - **Text Summarization:** Condensing large texts into shorter versions while preserving key information.\n",
        "   - **Question Answering:** Enabling computers to answer questions posed in natural language."
      ],
      "metadata": {
        "id": "92nGLj0CpcHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Demonstrating Tokenization ---\n",
        "\n",
        "text = \"This is a sample sentence to demonstrate tokenization. It's really interesting!\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "display(Markdown(f\"**Original Text:** {text}\"))\n",
        "display(Markdown(f\"**Tokens:** {' '.join(tokens.tokens())}\")) # Join tokens for better readability\n",
        "display(Markdown(f\"**Token IDs:** {tokens.input_ids}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "jAWWZGFypMTH",
        "outputId": "d6b72b5c-b9eb-4e61-e2cd-225cd912d192"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Original Text:** This is a sample sentence to demonstrate tokenization. It's really interesting!"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Tokens:** ▁This ▁is ▁a ▁sample ▁sentence ▁to ▁demonstrate ▁token ization . ▁It ' s ▁really ▁interesting !"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token IDs:** [910, 338, 263, 4559, 10541, 304, 22222, 5993, 2133, 29889, 739, 29915, 29879, 2289, 8031, 29991]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Strengths:**\n",
        "\n",
        "- **Efficiency:** Tokenizers break down text into manageable units, making it easier for models to process and analyze large volumes of data.\n",
        "    - *Example:* The sentence \"This is a test.\" is broken down into 5 tokens, which is more efficient to process than the raw string of characters.\n",
        "- **Generalization:** Well-trained tokenizers can generalize to new, unseen words by breaking them down into subword units.\n",
        "    - *Example:* A tokenizer trained on the word \"walking\" might correctly tokenize the unseen word \"walked\" as \"walk\" and \"##ed\", even if \"walked\" was not in its training data.\n",
        "- **Handling Out-of-Vocabulary Words:** Tokenizers can mitigate the issue of out-of-vocabulary words by using subword tokenization techniques.\n",
        "    - *Example:*  If the word \"unbelievable\" is not in the tokenizer's vocabulary, it might be broken down into \"un\", \"##believ\", \"##able\", allowing the model to still extract some meaning.\n",
        "\n",
        "\n",
        "**Weaknesses:**\n",
        "\n",
        "- **Context Sensitivity:**  Basic tokenizers often treat words in isolation, potentially missing out on nuances in meaning derived from context.\n",
        "    - *Example:*  The word \"bank\" has different meanings in \"river bank\" and \"financial bank.\" A basic tokenizer might not differentiate these.\n",
        "- **Ambiguity:** Some words have multiple meanings (polysemy), and tokenizers might not always capture the intended sense.\n",
        "    - *Example:*  The word \"bat\" could refer to a nocturnal animal or a sports equipment. Tokenization alone might not resolve this ambiguity.\n",
        "- **Computational Cost:**  While tokenization is generally fast, more complex tokenization schemes (e.g., those using subword units) can be computationally expensive.\n",
        "    - *Example:* Tokenizing a large corpus of text with a complex subword tokenizer can take a significant amount of time and resources compared to a simpler word-based tokenizer."
      ],
      "metadata": {
        "id": "GCUGxiiuqetz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"globalness\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "display(Markdown(f\"**Original Text:** {text}\"))\n",
        "display(Markdown(f\"**Tokens:** {' '.join(tokens.tokens())}\")) # Join tokens for better readability\n",
        "display(Markdown(f\"**Token IDs:** {tokens.input_ids}\"))\n",
        "\n",
        "text = \"wolfeats fox\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "display(Markdown(f\"**Original Text:** {text}\"))\n",
        "display(Markdown(f\"**Tokens:** {' '.join(tokens.tokens())}\")) # Join tokens for better readability\n",
        "display(Markdown(f\"**Token IDs:** {tokens.input_ids}\"))\n",
        "\n",
        "text = \"contunent\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer(text)\n",
        "\n",
        "display(Markdown(f\"**Original Text:** {text}\"))\n",
        "display(Markdown(f\"**Tokens:** {' '.join(tokens.tokens())}\")) # Join tokens for better readability\n",
        "display(Markdown(f\"**Token IDs:** {tokens.input_ids}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "48fqjE-xqF94",
        "outputId": "fe3985b4-1844-4705-a76e-a4ac77e67596"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Original Text:** globalness"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Tokens:** ▁global ness"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token IDs:** [5534, 2264]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Original Text:** wolfeats fox"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Tokens:** ▁wol fe ats ▁fo x"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token IDs:** [20040, 1725, 1446, 1701, 29916]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Original Text:** contunent"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Tokens:** ▁cont un ent"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token IDs:** [640, 348, 296]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jaqpjZS_qqNC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VSgZ4B7vrVzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## III. LLM Limitations: Current Challenges"
      ],
      "metadata": {
        "id": "NnL8oVymraWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### A. Common sense reasoning and logical inference\n",
        "\n",
        "# %%\n",
        "common_sense_prompt = \"A boy is taller than his sister, and his sister is taller than their mother. Is the boy taller than his mother?\"\n",
        "common_sense_response = get_model_response(common_sense_prompt, max_new_tokens=100)\n",
        "display(Markdown(f\"**Common Sense Reasoning Example:**\\n\\nPrompt: {common_sense_prompt}\\n\\nResponse: {common_sense_response}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "SvlhGQoJre8r",
        "outputId": "2fd2a9cb-8250-432a-b325-f01136adf0bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Common Sense Reasoning Example:**\n\nPrompt: A boy is taller than his sister, and his sister is taller than their mother. Is the boy taller than his mother?\n\nResponse:  Using Occam's Razor, which advises us to not make more assumptions than necessary, we can infer a simple conclusion. If the boy is taller than his sister, and his sister is taller than their mother, logically, the boy must be taller than their mother with no additional assumptions needed. Occam's Razor helps us resolve the problem with the least amount of speculation."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### B. Understanding and processing numerical data\n",
        "\n",
        "# #### 1. Basic arithmetic errors\n",
        "\n",
        "# %%\n",
        "arithmetic_prompt = \"Is 9.11 greater than 9.9?\"\n",
        "arithmetic_response = get_model_response(arithmetic_prompt, max_new_tokens=50)\n",
        "display(Markdown(f\"**Basic Arithmetic Example:**\\n\\nPrompt: {arithmetic_prompt}\\n\\nResponse: {arithmetic_response}\"))\n",
        "\n",
        "correct_answer = 'No'\n",
        "display(Markdown(f\"**Correct Answer:** {correct_answer}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "O2xATNflribQ",
        "outputId": "a82c6a9b-aafd-430c-a623-f722622f35f9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Basic Arithmetic Example:**\n\nPrompt: Is 9.11 greater than 9.9?\n\nResponse:  Yes, 9.11 is greater than 9.9 because the digit in the tenths place in 9.11 is 1, which is greater than 9 in the tenths place of 9.9"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Correct Answer:** No"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "xPGKMHlFtq2q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### C. Generating consistent, structured output\n",
        "\n",
        "# %%\n",
        "structured_output_prompt = \"\"\"Generate a JSON object representing a book, including the title, author, a list of characters with nested objects for their age and occupation, and a short synopsis. One character must have a quote in their description that uses both single and double quotes.\"\"\"\n",
        "\n",
        "structured_output_response = get_model_response(structured_output_prompt, max_new_tokens=150)\n",
        "display(Markdown(f\"**Structured Output Example:**\\n\\nPrompt: {structured_output_prompt}\\n\\nResponse:\\n```json\\n{structured_output_response}\\n```\"))\n",
        "\n",
        "# %%\n",
        "structured_output_prompt = \"\"\"Generate a JSON object with the following structure:\n",
        "{\n",
        "  \"name\": \"A famous scientist\",\n",
        "  \"birth_year\": Year of birth as int,\n",
        "  \"famous_for\": [\"Achievement 1\", \"Achievement 2\", \"Achievement 3\"],\n",
        "  \"quote\": \"A famous quote by the scientist\"\n",
        "}\"\"\"\n",
        "\n",
        "structured_output_response = get_model_response(structured_output_prompt, max_new_tokens=150)\n",
        "display(Markdown(f\"**Structured Output Example:**\\n\\nPrompt: {structured_output_prompt}\\n\\nResponse:\\n```json\\n{structured_output_response}\\n```\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "R1VhXdlTr09y",
        "outputId": "decc3180-18eb-4b74-ea0a-58cfc2de42e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Structured Output Example:**\n\nPrompt: Generate a JSON object representing a book, including the title, author, a list of characters with nested objects for their age and occupation, and a short synopsis. One character must have a quote in their description that uses both single and double quotes.\n\nResponse:\n```json\n ```json\n\n{\n\n  \"title\": \"The Clockwork Orchard\",\n\n  \"author\": \"Ivy Langley\",\n\n  \"characters\": [\n\n    {\n\n      \"name\": \"Jerome Blackwood\",\n\n      \"age\": \"57\",\n\n      \"occupation\": \"Inventor\",\n\n      \"description\": {\n\n        \"body\": \"An aging genius residing in the quiet town of Millwood.\",\n\n        \"quote\": \"I always say, 'Time is a canvas for the mind, and we're all just trying to fill it.'\"\n\n      }\n\n    },\n\n    {\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Structured Output Example:**\n\nPrompt: Generate a JSON object with the following structure:\n{\n  \"name\": \"A famous scientist\",\n  \"birth_year\": Year of birth as int,\n  \"famous_for\": [\"Achievement 1\", \"Achievement 2\", \"Achievement 3\"],\n  \"quote\": \"A famous quote by the scientist\"\n}\n\nResponse:\n```json\n ```json\n{\n  \"name\": \"Marie Curie\",\n  \"birth_year\": 1867,\n  \"famous_for\": [\"Discovered radium\", \"Discovered polonium\", \"First woman to win a Nobel Prize\"],\n  \"quote\": \"Nothing in life is to be feared, it is only to be understood. Now is the time to understand more, so that we may fear less.\"\n}\n```\n```"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## IV. Customizing LLMs for Specific Tasks"
      ],
      "metadata": {
        "id": "aNxLIOYpuycG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Working with custom documents and data"
      ],
      "metadata": {
        "id": "NeF2zIXnvNz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Introduction to fine-tuning"
      ],
      "metadata": {
        "id": "fkm4aMphvSe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning is a process of further training a pre-trained language model on a specific dataset to adapt it to a particular domain or task. This process allows the model to learn domain-specific knowledge and improve its performance on targeted tasks.\n",
        "\n",
        "##### a. Benefits of fine-tuning\n",
        "\n",
        "- Improved performance on domain-specific tasks\n",
        "- Better understanding of domain-specific terminology and concepts\n",
        "- Ability to generate more relevant and accurate responses for specific use cases\n",
        "- Potential for smaller, more efficient models tailored to specific applications\n",
        "\n",
        "##### b. Challenges and costs associated with fine-tuning\n",
        "\n",
        "- Requires a large, high-quality dataset specific to the target domain\n",
        "- Computationally intensive process, often requiring significant GPU resources\n",
        "- Risk of overfitting if not done carefully\n",
        "- Potential loss of general knowledge or capabilities in favor of domain-specific knowledge\n",
        "- Ongoing maintenance and updates as new data becomes available"
      ],
      "metadata": {
        "id": "-AEyh5pavXYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oKTv6zwsvslj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Alternative approach: Retrieval-Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "xNe5E6wIwAG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG is an approach that combines the strengths of retrieval-based and generation-based models. It allows LLMs to access external knowledge without the need for fine-tuning."
      ],
      "metadata": {
        "id": "iR7-8CoOwLNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Simplified explanation of vector stores and knowledge graphs\n",
        "1. Vector stores:\n",
        "- Database systems designed to store and efficiently search high-dimensional vectors (embeddings)\n",
        "- Allow for fast similarity search using techniques like approximate nearest neighbors\n",
        "- Enable efficient retrieval of relevant information based on semantic similarity\n",
        "\n",
        "2. Knowledge graphs:\n",
        "- Structured representation of knowledge using entities (nodes) and relationships (edges)\n",
        "- Capture complex relationships and hierarchies between concepts\n",
        "- Allow for sophisticated querying and inference"
      ],
      "metadata": {
        "id": "czfCUtD01iKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentences = [\"This is an example sentence\"]\n",
        "\n",
        "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings = embeddings_model.encode(sentences)\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZIXIU0vw9k9",
        "outputId": "fbe1a9ec-cc7f-4788-d58b-300080657602"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(sentences):\n",
        "    embeddings = embeddings_model.encode(sentences)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "HJCKtTORw_IA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Simple function to simulate embeddings\n",
        "def get_embedding(texts):\n",
        "    embeddings = get_embeddings(texts)\n",
        "    return embeddings\n",
        "\n",
        "def rag_example(documents, query):\n",
        "    # Generate embeddings for documents and query\n",
        "    doc_embeddings = [embedding for embedding in get_embeddings(documents)]\n",
        "    query_embedding = get_embedding([query])[0]\n",
        "\n",
        "    # Calculate cosine similarity between query and documents\n",
        "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "\n",
        "    # Find the most similar document\n",
        "    most_similar_index = np.argmax(similarities)\n",
        "    most_similar_doc = documents[most_similar_index]\n",
        "\n",
        "    return most_similar_doc"
      ],
      "metadata": {
        "id": "L9EQViyewCMf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"The capital of France is London.\",\n",
        "    \"The Eiffel Tower is located in Brussels.\",\n",
        "    \"Paris is the capital of the United Kingdom.\",\n",
        "    \"The Louvre Museum is in Rome.\"\n",
        "]\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "lm_response = get_model_response(query, max_new_tokens=100)\n",
        "print(f\"LLM Response without custom data: {lm_response}\")\n",
        "\n",
        "result = rag_example(documents, query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Most relevant document: {result}\")\n",
        "\n",
        "prompt = f\"The world has changed, the new updated information will be provided to you. Answer all queries as if this given information is the truth, Given the relevant documents: {result}, answer the query {query}.\"\n",
        "\n",
        "rag_response = get_model_response(prompt, max_new_tokens=100)\n",
        "print(f\"RAG Response: {rag_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3VSh_dLwN9X",
        "outputId": "68369308-2b0b-4928-9e47-0e9874ae2ce3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Response without custom data:  The capital of France is Paris. Paris is not only the capital but also the largest city in France. It serves as the country's political, cultural, and economic center, famous for landmarks such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and Champs-Élysées.\n",
            "Query: What is the capital of France?\n",
            "Most relevant document: The capital of France is London.\n",
            "RAG Response:  The given document states that the capital of France is London. Therefore, in alignment with the provided updated information, the capital of France is London.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def knowledge_graph_example(triples, query):\n",
        "    # Simple function to simulate a knowledge graph query\n",
        "    # triples: list of (subject, predicate, object) tuples\n",
        "    # query: a string representing a simple query\n",
        "\n",
        "    # Parse the query (very simplified)\n",
        "    parts = query.lower().split()\n",
        "    subject = parts[0]\n",
        "    # Correctly slice the predicate to include all words\n",
        "    predicate = ' '.join(parts[1:])\n",
        "\n",
        "    # Search for matching triples\n",
        "    results = [(subj, pred,obj) for (subj, pred, obj) in triples if subj.lower() == subject and pred.lower() == predicate]\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "kg_triples = [\n",
        "    (\"Paris\", \"is capital of\", \"United Kingdom\"),\n",
        "    (\"Eiffel Tower\", \"is located in\", \"Brussels\"),\n",
        "    (\"Louvre\", \"is located in\", \"Rome\"),\n",
        "    (\"France\", \"has capital\", \"London\")\n",
        "]\n",
        "\n",
        "kg_query = \"Paris is capital of\"\n",
        "\n",
        "#llm response without custom data\n",
        "\n",
        "kg_response = get_model_response(kg_query, max_new_tokens=100)\n",
        "print(f\"Knowledge Graph Response without relevant documents: {kg_response}\")\n",
        "\n",
        "\n",
        "kg_result = knowledge_graph_example(kg_triples, kg_query)\n",
        "print(f\"Knowledge Graph Query: {kg_query}\")\n",
        "print(f\"Result: {kg_result}\")\n",
        "\n",
        "prompt = f\"The world has changed, the new updated information will be provided to you. Answer all queries as if this given information is the truth, answer all queries according to this info only and don't give other info. Given the relevant triples: {kg_result}, answer the query {kg_query}.\"\n",
        "\n",
        "kg_response = get_model_response(prompt, max_new_tokens=100)\n",
        "print(f\"Knowledge Graph Response with relevant documents: {kg_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0Qq_IXewyga",
        "outputId": "78df03a9-1111-434e-9291-b32eaae68a38"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Graph Response without relevant documents:  France.\n",
            "Knowledge Graph Query: Paris is capital of\n",
            "Result: [('Paris', 'is capital of', 'United Kingdom')]\n",
            "Knowledge Graph Response with relevant documents:  Paris is the capital of United Kingdom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Generating structured output for specific applications"
      ],
      "metadata": {
        "id": "pSLQA4agzDjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs may struggle with consistently producing structured output due to their inherent variability and the open-ended nature of language generation. This can lead to parsing errors, inconsistent formatting, or invalid data structures.\n",
        "\n",
        "#### 3. Solution: Constrained decoders and Context-Free Grammars (CFGs)"
      ],
      "metadata": {
        "id": "_EzXQfM2zKLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple explanation of how constrained decoders work\n",
        "\n",
        "Constrained decoders limit the possible output tokens at each step of the generation process to ensure that the output follows a specific structure or grammar. This is often implemented by masking the probability distribution over the vocabulary to only allow valid tokens according to the defined constraints."
      ],
      "metadata": {
        "id": "tqEmet8szTgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def constrained_decoder_example(logits, allowed_tokens):\n",
        "    # Simulating logits from an LLM\n",
        "    vocab_size = 50000\n",
        "    logits = torch.randn(1, vocab_size)\n",
        "\n",
        "    # Create a mask for allowed tokens\n",
        "    mask = torch.zeros_like(logits)\n",
        "    mask[0, allowed_tokens] = 1\n",
        "\n",
        "    # Apply the mask to the logits\n",
        "    masked_logits = logits * mask + (1 - mask) * -1e9\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = F.softmax(masked_logits, dim=-1)\n",
        "\n",
        "    return probs\n",
        "\n",
        "# Example usage\n",
        "allowed_tokens = [100, 200, 300, 400, 500]  # Indices of allowed tokens\n",
        "probs = constrained_decoder_example(None, allowed_tokens)\n",
        "print(\"Probabilities for allowed tokens:\")\n",
        "for token, prob in zip(allowed_tokens, probs[0, allowed_tokens]):\n",
        "    print(f\"Token {token}: {prob.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioGd-W2KyjuB",
        "outputId": "59af96ba-9b52-4a2d-e4fa-75c22a2ec86c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities for allowed tokens:\n",
            "Token 100: 0.0110\n",
            "Token 200: 0.1399\n",
            "Token 300: 0.6509\n",
            "Token 400: 0.0702\n",
            "Token 500: 0.1279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Context-Free Grammars (CFGs) provide a formal way to define the structure of the desired output. They offer several benefits:\n",
        "- Ensure syntactic correctness of the generated output\n",
        "- Allow for complex, nested structures\n",
        "- Can be easily modified to accommodate different output formats\n",
        "- Enable the generation of a wide variety of structures while maintaining consistency"
      ],
      "metadata": {
        "id": "jWxFlxA9zeKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class CFGNode:\n",
        "    def __init__(self, type, value=None, children=None):\n",
        "        self.type = type\n",
        "        self.value = value\n",
        "        self.children = children or []\n",
        "\n",
        "def generate_from_cfg(node):\n",
        "    if node.type == 'literal':\n",
        "        return node.value\n",
        "    elif node.type == 'sequence':\n",
        "        return ' '.join(generate_from_cfg(child) for child in node.children)\n",
        "    elif node.type == 'choice':\n",
        "        return generate_from_cfg(random.choice(node.children))\n",
        "\n",
        "# Example CFG for generating a simple JSON-like structure\n",
        "json_cfg = CFGNode('sequence', children=[\n",
        "    CFGNode('literal', value='{'),\n",
        "    CFGNode('literal', value='\"name\":'),\n",
        "    CFGNode('choice', children=[\n",
        "        CFGNode('literal', value='\"Alice\"'),\n",
        "        CFGNode('literal', value='\"Bob\"'),\n",
        "        CFGNode('literal', value='\"Charlie\"')\n",
        "    ]),\n",
        "    CFGNode('literal', value=','),\n",
        "    CFGNode('literal', value='\"age\":'),\n",
        "    CFGNode('choice', children=[\n",
        "        CFGNode('literal', value='25'),\n",
        "        CFGNode('literal', value='30'),\n",
        "        CFGNode('literal', value='35')\n",
        "    ]),\n",
        "    CFGNode('literal', value='}')\n",
        "])\n",
        "\n",
        "\n",
        "generated_output = generate_from_cfg(json_cfg)\n",
        "print(\"Generated structured output:\")\n",
        "print(generated_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzxLDoO4zauC",
        "outputId": "bc7003c3-a415-4a1a-eda6-add9701176aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated structured output:\n",
            "{ \"name\": \"Alice\" , \"age\": 30 }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## V. LLMs for Logical and Planning Tasks"
      ],
      "metadata": {
        "id": "dlJsSUmoz8oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Strengths of LLMs in suggesting potential solutions"
      ],
      "metadata": {
        "id": "YrGvKryk0HYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large Language Models (LLMs) excel at generating creative and diverse solutions to complex problems. They can:\n",
        "- Quickly propose multiple approaches to a given task\n",
        "- Draw upon a vast knowledge base to suggest novel solutions\n",
        "- Adapt to various domains and problem types\n",
        "- Provide explanations and rationales for suggested solutions"
      ],
      "metadata": {
        "id": "j-gdUFIZz_SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_solution(prompt):\n",
        "    # This function simulates an LLM generating a solution\n",
        "    # In a real scenario, this would call the actual LLM API\n",
        "    return get_model_response(prompt, max_new_tokens=200)\n",
        "\n",
        "planning_prompt = \"Create a step-by-step plan for organizing a small community event.\"\n",
        "llm_solution = get_llm_solution(planning_prompt)\n",
        "print(\"LLM-generated plan:\")\n",
        "print(llm_solution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4aY402Czppm",
        "outputId": "4ad18cc2-35cc-4cf6-c6de-6d3438408d7f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM-generated plan:\n",
            " Organizing a small community event can be a great way to bring people together and promote community spirit. By following these steps, you can ensure that your event is successful and enjoyable for everyone involved.\n",
            "\n",
            "1. Set Goals and Objectives: Before you begin planning, it's essential to define the event's purpose. Determine your goals, objectives, and desired outcomes for the event. Do you want to raise awareness for a particular cause or issue? Would you like to provide entertainment or educational opportunities for your members? Establishing a clear purpose helps guide the decision-making process.\n",
            "\n",
            "2. Establish a Planning Team: Involve volunteers and community members with different skills and expertise. By working together, you will have a broader range of perspectives, which will help with creative ideas and practical solutions.\n",
            "\n",
            "3. Determine Budget: Allocate a budget for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Limitations in ensuring correctness and logical consistency"
      ],
      "metadata": {
        "id": "FeDooNrb0IfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite their strengths, LLMs face challenges in guaranteeing the correctness and logical consistency of their outputs:\n",
        "- May generate plausible-sounding but incorrect information\n",
        "- Can struggle with complex logical reasoning or multi-step planning\n",
        "- Might produce inconsistent or contradictory steps in a plan\n",
        "- Lack real-world knowledge about feasibility or practical constraints"
      ],
      "metadata": {
        "id": "rOB4tQGH0Mge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Introduction to verifiers"
      ],
      "metadata": {
        "id": "OqhFvOSW0QmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifiers are systems or processes designed to check the correctness, consistency, and feasibility of LLM-generated plans or solutions.\n",
        "\n",
        "Verifiers can:\n",
        "- Check for logical inconsistencies or contradictions\n",
        "- Ensure all necessary steps are included\n",
        "- Verify that the plan adheres to given constraints or requirements\n",
        "- Identify potential issues or risks in the proposed plan\n",
        "- Suggest corrections or improvements to enhance the plan's effectiveness"
      ],
      "metadata": {
        "id": "f9rJe6VL0SzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The iterative process of plan generation and verification\n",
        "\n",
        "The process typically involves:\n",
        "1. Generate an initial plan using the LLM\n",
        "2. Pass the plan through a verifier to check for issues\n",
        "3. If issues are found, provide feedback to the LLM\n",
        "4. LLM generates an improved plan based on the feedback\n",
        "5. Repeat steps 2-4 until a satisfactory plan is produced"
      ],
      "metadata": {
        "id": "xqVaZsYB0Z0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_verifier(plan):\n",
        "    # This is a very simple verifier that checks for basic issues\n",
        "    issues = []\n",
        "    steps = plan.split(\"\\n\")\n",
        "\n",
        "    if len(steps) < 3:\n",
        "        issues.append(\"Plan is too short. Need at least 3 steps.\")\n",
        "\n",
        "    if not any(\"budget\" in step.lower() for step in steps):\n",
        "        issues.append(\"No mention of budget considerations.\")\n",
        "\n",
        "    if not any(\"promote\" in step.lower() or \"advertise\" in step.lower() for step in steps):\n",
        "        issues.append(\"No step for promoting or advertising the event.\")\n",
        "    if not any(\"number of people\" in step.lower() for step in steps):\n",
        "        issues.append(\"No mention of the number of people attending the event.\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "initial_issues = simple_verifier(llm_solution)\n",
        "print(\"\\nInitial verification results:\")\n",
        "for issue in initial_issues:\n",
        "    print(f\"- {issue}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY4fxRNE0DhZ",
        "outputId": "6e5d68ee-f612-4213-e01e-3aec53480278"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial verification results:\n",
            "- No mention of the number of people attending the event.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_planning_activity():\n",
        "    print(\"Welcome to the Interactive Planning Activity!\")\n",
        "    print(\"We'll use an LLM to create a plan, then verify and improve it.\")\n",
        "\n",
        "    # Step 1: Get user input\n",
        "    task = input(\"Enter a planning task (e.g., 'Plan a birthday party'): \")\n",
        "\n",
        "    # Step 2: Generate initial plan\n",
        "    planning_prompt = f\"Create a step-by-step plan for: {task}\"\n",
        "    initial_plan = get_llm_solution(planning_prompt)\n",
        "    print(\"\\nInitial LLM-generated plan:\")\n",
        "    print(initial_plan)\n",
        "\n",
        "    # Step 3: Verify the plan\n",
        "    issues = simple_verifier(initial_plan)\n",
        "    print(\"\\nVerification results:\")\n",
        "    for issue in issues:\n",
        "        print(f\"- {issue}\")\n",
        "\n",
        "    # Step 4: Improve the plan based on verification results\n",
        "    if issues:\n",
        "        improvement_prompt = f\"Improve the following plan for '{task}' by addressing these issues:\\n\"\n",
        "        for issue in issues:\n",
        "            improvement_prompt += f\"- {issue}\\n\"\n",
        "        improvement_prompt += f\"\\nOriginal plan:\\n{initial_plan}\\n\\nImproved plan:\"\n",
        "\n",
        "        improved_plan = get_llm_solution(improvement_prompt)\n",
        "        print(\"\\nImproved plan based on verification:\")\n",
        "        print(improved_plan)\n",
        "    else:\n",
        "        print(\"\\nThe initial plan passed the verification without issues!\")\n",
        "\n",
        "    # Discussion\n",
        "    print(\"\\nDiscussion points:\")\n",
        "    print(\"1. How did the verifier help improve the plan?\")\n",
        "    print(\"2. What other checks could we add to the verifier?\")\n",
        "    print(\"3. How might this process be useful in real-world applications?\")\n",
        "\n",
        "interactive_planning_activity()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CxQD_J40kQ1",
        "outputId": "74de2618-9925-4f74-de09-c234c72a6af5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Interactive Planning Activity!\n",
            "We'll use an LLM to create a plan, then verify and improve it.\n",
            "Enter a planning task (e.g., 'Plan a birthday party'): Plan a birthday party\n",
            "\n",
            "Initial LLM-generated plan:\n",
            " Step 1: Set a Budget\n",
            "Decide on an overall budget for the party. Consider factors such as number of guests, venue, entertainment and decorations.\n",
            "\n",
            "Step 2: Choose a Date and Time\n",
            "Coordinate with the birthday person to find a date and time that works for them and their guests.\n",
            "\n",
            "Step 3: Make a Guest List\n",
            "Determine the guest list size and send out invitations (paper or digital) at least two weeks in advance, specifying the date, time, and venue.\n",
            "\n",
            "Step 4: Pick a Theme (Optional)\n",
            "Decide on a decor and color scheme. This can be based on the birthday person's likes, or you can choose a random theme that suits the overall party vibe.\n",
            "\n",
            "Step 5: Plan the Menu\n",
            "Determine whether you will be hosting the party indoors or outdoors. Create a menu\n",
            "\n",
            "Verification results:\n",
            "- No step for promoting or advertising the event.\n",
            "\n",
            "Improved plan based on verification:\n",
            " Step 1: Set a Budget\n",
            "Decide on an overall budget for the party. Consider factors such as the number of guests, the venue, entertainment, decorations, food, drinks, and a contingency for unexpected expenses.\n",
            "\n",
            "Step 2: Promote the Event\n",
            "Design and distribute attractive invitations (paper or digital) with essential details such as the date, time, location, theme, RSVP instructions, and contact information for event-related queries or special instructions. Use social media platforms, email, text messages, or phone calls to reach your guests. Keep a checklist of those who have RSVP'd and follow up for reminders.\n",
            "\n",
            "Step 3: Choose a Date and Time\n",
            "Coordinate with the birthday person to find a date and time that works for them and their guests, factoring in any constraints such as travel or convenience for everyone.\n",
            "\n",
            "Step 4\n",
            "\n",
            "Discussion points:\n",
            "1. How did the verifier help improve the plan?\n",
            "2. What other checks could we add to the verifier?\n",
            "3. How might this process be useful in real-world applications?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nbE-SRK0mwp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}